# -*- coding: utf-8 -*-
"""PDF_RAG_BOT.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1doFqR3MSFBIbzGvNG4qHgKY8ZshgeQXq
"""

!pip install gradio
!pip install langchain langchain_google_genai langchain_community langchain_core
!pip install chromadb
!pip install pypdf
!pip install PyPDF2

import gradio as gr
import os
import hashlib
import tempfile
import logging
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.document_loaders import PyPDFLoader
from langchain_google_genai import ChatGoogleGenerativeAI, GoogleGenerativeAIEmbeddings
from langchain_community.vectorstores import Chroma
from langchain_core.messages import HumanMessage, AIMessage
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain.chains import create_history_aware_retriever, create_retrieval_chain
from langchain.chains.combine_documents import create_stuff_documents_chain

persistent_directory = '/content/new_db'
embeddings = GoogleGenerativeAIEmbeddings(model="models/embedding-001")


logger = logging.getLogger(__name__)
logging.basicConfig(level=logging.INFO)

def compute_file_hash(file_path):
    BUF_SIZE = 65536
    sha256 = hashlib.sha256()
    try:
        with open(file_path, 'rb') as f:
            while chunk := f.read(BUF_SIZE):
                sha256.update(chunk)
    except Exception as e:
        logger.error(f"Error computing hash for file {file_path}: {e}")
    return sha256.hexdigest()

def process_pdfs(files):
    existing_hashes = set()
    hash_file_path = os.path.join(persistent_directory, 'hashes.txt')

    if os.path.exists(hash_file_path):
        with open(hash_file_path, 'r') as f:
            existing_hashes = set(line.strip() for line in f.readlines())

    documents = []

    try:
        for file in files:
            with tempfile.NamedTemporaryFile(delete=False, suffix=".pdf") as temp_file:
                temp_file.write(file)
                temp_file_path = temp_file.name

            file_hash = compute_file_hash(temp_file_path)

            if file_hash in existing_hashes:
                logger.info(f"Skipping {file_hash} as it is already in the database.")
                continue

            logger.info(f"Temporary file saved at: {temp_file_path}")

            loader = PyPDFLoader(temp_file_path)
            book_docs = loader.load()
            for doc in book_docs:
                doc.metadata = {'source': file_hash, 'hash': file_hash}
                documents.append(doc)

            existing_hashes.add(file_hash)

        text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=20)
        docs = text_splitter.split_documents(documents)

        db = Chroma(persist_directory=persistent_directory, embedding_function=embeddings)
        if documents:
            db.add_documents(docs)
            with open(hash_file_path, 'a') as f:
                for doc in documents:
                    f.write(f"{doc.metadata['hash']}\n")

        return "Files processed and stored. You can now ask questions."

    except Exception as e:
        logger.error(f"Error processing files: {e}")
        return f"Error processing files: {e}"

def get_answer(query):
    try:
        db = Chroma(persist_directory=persistent_directory, embedding_function=embeddings)

        retriever = db.as_retriever(
            search_type="similarity",
            search_kwargs={"k": 3},
        )

        contextualize_q_system_prompt = (
            "Given a chat history and the latest user question "
            "which might reference context in the chat history, "
            "formulate a standalone question which can be understood "
            "without the chat history. Do NOT answer the question, just "
            "reformulate it if needed and otherwise return it as is."
        )

        contextualize_q_prompt = ChatPromptTemplate.from_messages(
            [
                ("system", contextualize_q_system_prompt),
                MessagesPlaceholder("chat_history"),
                ("human", "{input}"),
            ]
        )

        history_aware_retriever = create_history_aware_retriever(
            model, retriever, contextualize_q_prompt
        )

        qa_system_prompt = (
            "You are an assistant for question-answering tasks. Use "
            "the following pieces of retrieved context to answer the "
            "question. If you don't know the answer, just say that you "
            "don't know. Use three sentences maximum and keep the answer "
            "concise."
            "\n\n"
            "{context}"
        )

        qa_prompt = ChatPromptTemplate.from_messages(
            [
                ("system", qa_system_prompt),
                MessagesPlaceholder("chat_history"),
                ("human", "{input}"),
            ]
        )

        question_answer_chain = create_stuff_documents_chain(model, qa_prompt)

        rag_chain = create_retrieval_chain(history_aware_retriever, question_answer_chain)

        result = rag_chain.invoke({"input": query, "chat_history": []})
        return result["answer"]

    except Exception as e:
        logger.error(f"Error retrieving answer: {e}")
        return f"Error retrieving answer: {e}"

with gr.Blocks() as demo:
    gr.Markdown("# AI-Enhanced PDF Chat Interface")
    with gr.Row():
        with gr.Column():
            file_input = gr.File(label="Upload PDF Files", type="binary", file_count="multiple")
            file_output = gr.Textbox(label="File Processing Status", placeholder="Status will be shown here")
        with gr.Column():
            query_input = gr.Textbox(label="Ask a Question", placeholder="Enter your question here")
            with gr.Row():
              submit_button = gr.Button("Submit Query")
              clear_button = gr.Button("Clear Query")
            query_output = gr.Textbox(label="AI Response", placeholder="AI Response")

    def clear_query():
        return "", ""

    file_input.upload(
        process_pdfs,
        inputs=file_input,
        outputs=file_output
    )

    submit_button.click(
        get_answer,
        inputs=query_input,
        outputs=query_output
    )

    clear_button.click(
        clear_query,
        inputs=None,
        outputs=[query_input, query_output]
    )



demo.launch()

